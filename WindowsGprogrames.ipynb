{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffc1302-bb30-4860-8c1b-0bf2402197fb",
   "metadata": {},
   "source": [
    "# Implement a basic GAN architecture (generator + discriminator) and train it on MNIST images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb766cf-4613-44f3-bd3c-034fc9030980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(7*7*128, input_shape=(latent_dim,)),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2DTranspose(1, 4, strides=2, padding='same', activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(64, 4, strides=2, padding='same', input_shape=(28, 28, 1)),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2D(128, 4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "latent_dim = 100\n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "g_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "d_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_images = generator(noise, training=True)\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "        d_loss = cross_entropy(tf.ones_like(real_output), real_output) + \\\n",
    "                 cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    d_gradients = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    d_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        fake_images = generator(noise, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "        g_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    g_gradients = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    g_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "    return d_loss, g_loss\n",
    "\n",
    "def train(dataset, epochs, batch_size=128):\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataset:\n",
    "            d_loss, g_loss = train_step(batch)\n",
    "        print(f\"Epoch {epoch + 1}: D Loss: {d_loss.numpy():.4f}, G Loss: {g_loss.numpy():.4f}\")\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            generate_and_plot_images(generator, latent_dim)\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1024).batch(batch_size)\n",
    "\n",
    "def generate_and_plot_images(generator, latent_dim, n=16):\n",
    "    noise = tf.random.normal([n, latent_dim])\n",
    "    fake_images = generator(noise, training=False)\n",
    "    fake_images = fake_images.numpy().squeeze()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    for i in range(n):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(fake_images[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "train(train_dataset, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4407cd1-8fdd-4cb5-b3da-6fbf209cb0ac",
   "metadata": {},
   "source": [
    "# Experiment with DCGAN architecture on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ae1b9-b84f-492b-bcd5-2b4b3cc8b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_train = (x_train - 127.5) / 127.5\n",
    "\n",
    "BUFFER_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(4*4*512, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Reshape((4, 4, 512)))\n",
    "    model.add(layers.Conv2DTranspose(256, 4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv2DTranspose(128, 4, strides=2, padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Conv2DTranspose(3, 4, strides=2, padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, 4, strides=2, padding='same', input_shape=[32, 32, 3]))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Conv2D(128, 4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Conv2D(256, 4, strides=2, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\n",
    "\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        d_loss = discriminator_loss(real_output, fake_output)\n",
    "        g_loss = generator_loss(fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return d_loss, g_loss\n",
    "\n",
    "def generate_and_plot_images(model, epoch):\n",
    "    predictions = model(seed, training=False)\n",
    "    predictions = (predictions + 1) / 2.0\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Epoch {epoch}\")\n",
    "    plt.show()\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            d_loss, g_loss = train_step(image_batch)\n",
    "        print(f\"Epoch {epoch + 1}, D Loss: {d_loss.numpy():.4f}, G Loss: {g_loss.numpy():.4f}\")\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            generate_and_plot_images(generator, epoch + 1)\n",
    "\n",
    "train(train_dataset, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e836f4-9724-4253-af35-8b86412bdc83",
   "metadata": {},
   "source": [
    "# TEXT TO IMAGE AND IMAGE TO TEXT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d56956c-f5bd-41c7-974b-24e460c3822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install diffusers transformers accelerate safetensors pillow\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install --upgrade diffusers\n",
    "\n",
    "install the above packages if required.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "def text_to_image(prompt, output_dir=\"outputs\", num_images=1, steps=30, guidance_scale=7.5, seed=None):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Loading Stable Diffusion model on {device}...\")\n",
    "    try:\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\",\n",
    "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "            safety_checker=None\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading Stable Diffusion pipeline:\", e)\n",
    "        return\n",
    "    generator = torch.Generator(device=device).manual_seed(seed) if seed else None\n",
    "    results = []\n",
    "    for i in range(num_images):\n",
    "        print(f\"Generating image {i + 1} of {num_images}...\")\n",
    "        try:\n",
    "            image = pipe(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error during image generation: {e}\")\n",
    "            continue\n",
    "        file_path = os.path.join(output_dir, f\"text2img_{i + 1}.png\")\n",
    "        image.save(file_path)\n",
    "        results.append(file_path)\n",
    "    metadata = {\n",
    "        \"prompt\": prompt,\n",
    "        \"num_images\": num_images,\n",
    "        \"steps\": steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"seed\": seed,\n",
    "        \"device\": device,\n",
    "        \"outputs\": results\n",
    "    }\n",
    "    metadata_path = os.path.join(output_dir, \"text2img_metadata.json\")\n",
    "    try:\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(\"Error saving metadata:\", e)\n",
    "    print(\"Images saved to:\", results)\n",
    "\n",
    "def image_to_text(image_path, output_dir=\"outputs\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Loading image captioning model on {device}...\")\n",
    "    model_name = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "    try:\n",
    "        model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "        processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading image captioning model:\", e)\n",
    "        return\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image: {e}\")\n",
    "        return\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    try:\n",
    "        output_ids = model.generate(pixel_values, max_length=32, num_beams=4)\n",
    "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during caption generation: {e}\")\n",
    "        return\n",
    "    caption_file = os.path.join(output_dir, \"img2text_caption.txt\")\n",
    "    try:\n",
    "        with open(caption_file, \"w\") as f:\n",
    "            f.write(caption)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving caption: {e}\")\n",
    "    metadata = {\n",
    "        \"image_path\": image_path,\n",
    "        \"caption\": caption,\n",
    "        \"device\": device\n",
    "    }\n",
    "    metadata_path = os.path.join(output_dir, \"img2text_metadata.json\")\n",
    "    try:\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving metadata: {e}\")\n",
    "    print(\"Caption:\", caption)\n",
    "    print(\"Caption saved to:\", caption_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    choice = input(\"Enter choice (1 for Text-to-Image, 2 for Image-to-Text): \").strip()\n",
    "    if choice == \"1\":\n",
    "        print(\"\\n--- Text-to-Image ---\")\n",
    "        prompt = input(\"Enter your text prompt: \").strip()\n",
    "        num_images = int(input(\"Number of images to generate (default 1): \") or 1)\n",
    "        steps = int(input(\"Number of diffusion steps (default 30): \") or 30)\n",
    "        guidance_scale = float(input(\"Guidance scale (default 7.5): \") or 7.5)\n",
    "        seed_input = input(\"Random seed (leave blank for random): \").strip()\n",
    "        seed = int(seed_input) if seed_input else None\n",
    "        text_to_image(\n",
    "            prompt,\n",
    "            num_images=num_images,\n",
    "            steps=steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            seed=seed\n",
    "        )\n",
    "    elif choice == \"2\":\n",
    "        print(\"\\n--- Image-to-Text ---\")\n",
    "        image_path = input(\"Enter path to the image: \").strip()\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"Error: File not found.\")\n",
    "        else:\n",
    "            image_to_text(image_path)\n",
    "    else:\n",
    "        print(\"Invalid choice.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99411514-8b10-43e6-b01a-2f4fe1a63e51",
   "metadata": {},
   "source": [
    "# SPEECH TO TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea61481-3f8a-4f7f-9e3a-c1a1e6b32242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install openai-whisper\n",
    "!pip install whisper\n",
    "!pip install librosa\n",
    "\n",
    "install these packages if required.\n",
    "\"\"\"\n",
    "\n",
    "import whisper\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "MODEL_SIZE = \"base\"\n",
    "\n",
    "def transcribe_file(audio_path):\n",
    "    print(\"\\nLoading Whisper model...\")\n",
    "    model = whisper.load_model(MODEL_SIZE)\n",
    "    print(\"Model loaded. Transcribing audio file...\\n\")\n",
    "    audio, sr = sf.read(audio_path)\n",
    "    if sr != 16000:\n",
    "        import librosa\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "    audio = audio.astype(np.float32)\n",
    "    result = model.transcribe(audio, fp16=False, language=\"en\")\n",
    "    print(\"Transcription:\", result[\"text\"].strip())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = input(\"Enter path to the recorded audio file: \").strip()\n",
    "    transcribe_file(audio_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082495e-401e-4077-aa2e-8ce80c1fd4e3",
   "metadata": {},
   "source": [
    "# To implement the forward diffusion process on images (adding noise gradually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6e2e6-f0e4-4161-9e45-1cea5c59f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "image = x_train[0]\n",
    "\n",
    "T = 1000\n",
    "beta = np.linspace(1e-4, 0.02, T)\n",
    "alpha = 1.0 - beta\n",
    "alpha_hat = np.cumprod(alpha)\n",
    "\n",
    "def forward_diffusion_sample(x_0, t):\n",
    "    sqrt_alpha_hat = np.sqrt(alpha_hat[t])\n",
    "    sqrt_one_minus_alpha_hat = np.sqrt(1 - alpha_hat[t])\n",
    "    noise = np.random.randn(*x_0.shape)\n",
    "    x_t = sqrt_alpha_hat * x_0 + sqrt_one_minus_alpha_hat * noise\n",
    "    return x_t, noise\n",
    "\n",
    "def show_forward_diffusion(x_0, steps=[0, 100, 200, 500, 999]):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, t in enumerate(steps):\n",
    "        x_t, _ = forward_diffusion_sample(x_0, t)\n",
    "        plt.subplot(1, len(steps), i + 1)\n",
    "        plt.imshow(x_t.squeeze(), cmap='gray')\n",
    "        plt.title(f\"t = {t}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Forward Diffusion Process\")\n",
    "    plt.show()\n",
    "\n",
    "show_forward_diffusion(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1bd27-e4b9-4825-98c3-69a136f9cf60",
   "metadata": {},
   "source": [
    "# reverse diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7c635-01a1-4fae-9999-d948279cecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T = 1000\n",
    "beta = np.linspace(1e-4, 0.02, T)\n",
    "alpha = 1.0 - beta\n",
    "alpha_hat = np.cumprod(alpha)\n",
    "\n",
    "beta = tf.constant(beta, dtype=tf.float32)\n",
    "alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "alpha_hat = tf.constant(alpha_hat, dtype=tf.float32)\n",
    "\n",
    "def get_simple_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(28, 28, 2)),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        layers.Conv2D(1, 3, padding=\"same\")\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "noise_predictor = get_simple_model()\n",
    "\n",
    "def get_timestep_tensor(t, shape):\n",
    "    t_scaled = tf.fill(shape, tf.cast(t, tf.float32) / T)\n",
    "    return t_scaled[..., tf.newaxis]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x_0):\n",
    "    t = tf.random.uniform([], minval=1, maxval=T, dtype=tf.int32)\n",
    "    noise = tf.random.normal(tf.shape(x_0))\n",
    "    alpha_hat_t = tf.gather(alpha_hat, t)\n",
    "    sqrt_alpha_hat_t = tf.sqrt(alpha_hat_t)\n",
    "    sqrt_one_minus_alpha_hat_t = tf.sqrt(1 - alpha_hat_t)\n",
    "    x_t = sqrt_alpha_hat_t * x_0 + sqrt_one_minus_alpha_hat_t * noise\n",
    "    t_tensor = get_timestep_tensor(t, tf.shape(x_0)[:3])\n",
    "    x_input = tf.concat([x_t, t_tensor], axis=-1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_noise = noise_predictor(x_input)\n",
    "        loss = tf.reduce_mean(tf.square(noise - pred_noise))\n",
    "    grads = tape.gradient(loss, noise_predictor.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, noise_predictor.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def reverse_diffusion(x_T, steps=T, model=noise_predictor):\n",
    "    x_t = x_T\n",
    "    for t in reversed(range(1, steps)):\n",
    "        alpha_t = alpha[t]\n",
    "        alpha_hat_t = alpha_hat[t]\n",
    "        beta_t = beta[t]\n",
    "        t_tensor = get_timestep_tensor(t, tf.shape(x_t)[:3])\n",
    "        x_input = tf.concat([x_t, t_tensor], axis=-1)\n",
    "        pred_noise = model(x_input)\n",
    "        coef1 = 1 / tf.sqrt(alpha_t)\n",
    "        coef2 = (1 - alpha_t) / tf.sqrt(1 - alpha_hat_t)\n",
    "        x_t = coef1 * (x_t - coef2 * pred_noise)\n",
    "        if t > 1:\n",
    "            noise = tf.random.normal(tf.shape(x_t))\n",
    "            x_t += tf.sqrt(beta_t) * noise\n",
    "    return x_t\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    for i in range(0, 1024, 32):\n",
    "        batch = x_train[i:i+32]\n",
    "        loss = train_step(batch)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "x_T = tf.random.normal((1, 28, 28, 1))\n",
    "x_denoised = reverse_diffusion(x_T, steps=1000)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_T[0, ..., 0], cmap=\"gray\")\n",
    "plt.title(\"x_T (Noise)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(tf.clip_by_value(x_denoised[0, ..., 0], 0, 1), cmap=\"gray\")\n",
    "plt.title(\"x_0 (Denoised)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb9e0c-5b9c-48a3-9605-00726fd700b4",
   "metadata": {},
   "source": [
    "## fine-tune a pre-trained transformer model on a text dataset for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cdbe5-a6c1-43f4-9c8e-64794a0ddcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "texts = [\"Here is some example text.\", \"Another example sentence.\"]\n",
    "inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "labels = inputs.input_ids\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs.input_ids, labels))\n",
    "dataset = dataset.shuffle(100).batch(2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_ids, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss_value = outputs.loss\n",
    "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_value\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in dataset:\n",
    "        loss_value = train_step(batch[0], batch[1])\n",
    "    print(f\"Epoch {epoch+1} Loss: {loss_value.numpy():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
